<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Text-to-Pose Diffusion - Joel Markapudi</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f0;
        }

        .container {
            max-width: 920px;
            margin: 0 auto;
            padding: 50px 30px 40px 30px;
            background: #fafaf5;
            min-height: 100vh;
        }

        .back-link {
            display: inline-flex;
            align-items: center;
            color: #4a90e2;
            text-decoration: none;
            margin-bottom: 30px;
            font-size: 0.95rem;
            transition: color 0.3s;
        }

        .back-link:hover {
            color: #357abd;
        }

        h1 {
            font-size: 2.2rem;
            font-weight: 400;
            margin-bottom: 18px;
            color: #2c2c2c;
            line-height: 1.3;
        }

        .project-overview {
            color: #555;
            font-size: 1.05rem;
            margin-bottom: 25px;
            padding-bottom: 20px;
            border-bottom: 2px solid #e0e0d8;
            line-height: 1.7;
        }

        .highlights {
            margin-top: 25px;
        }

        .highlights ul {
            list-style: none;
            margin: 0;
            padding: 0;
        }

        .highlights li {
            margin-bottom: 16px;
            padding-left: 20px;
            position: relative;
            color: #555;
            line-height: 1.7;
        }

        .highlights li:before {
            content: "•";
            position: absolute;
            left: 0;
            color: #4a90e2;
            font-weight: bold;
            font-size: 1.2rem;
        }

        strong {
            color: #2c2c2c;
            font-weight: 500;
        }

        .external-links {
            margin-top: 35px;
            padding-top: 25px;
            border-top: 1px solid #e0e0d8;
        }

        .external-links h2 {
            font-size: 1.3rem;
            font-weight: 500;
            margin-bottom: 15px;
            color: #2c2c2c;
        }

        .external-links a {
            display: inline-block;
            margin-right: 20px;
            margin-bottom: 10px;
            color: #4a90e2;
            text-decoration: none;
            font-size: 1rem;
            transition: color 0.3s;
        }

        .external-links a:hover {
            color: #357abd;
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            .container {
                padding: 35px 20px;
            }

            h1 {
                font-size: 1.8rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <a href="index.html" class="back-link">← Back to Portfolio</a>
        
        <h1>Text-to-Pose Diffusion: CLIP-Conditioned 3D Pose Synthesis</h1>
        
        <div class="project-overview">
            Cross-modal diffusion model that maps free-form text into 3D human poses, integrating CLIP semantics with a UNet-based diffusion backbone. Focuses on anatomically consistent skeletons, kinematic chain reasoning, and pose–text alignment rather than pretty images, built in PyTorch with CLIP as the semantic encoder.
        </div>

        <div class="highlights">
            <ul>
                <li><strong>Cross-Modal Architecture:</strong> Hybrid CNN–Transformer UNet where CLIP text embeddings are projected into pose-space and injected via cross-attention at multiple resolution levels, enabling the model to condition on high-level actions (e.g., “running”, “reaching”) and finer body-part cues simultaneously.</li>

                <li><strong>Specialized Cross-Attention:</strong> Hierarchical semantic conditioning that routes different aspects of the text into different spatial scales: coarse layers focus on global posture and balance, while mid/fine layers emphasize limb orientation, hand/foot placement, and spatial relationships between joints.</li>

                <li><strong>Biomechanics & Physical Plausibility:</strong> Enforces bone-length consistency, joint-limit style constraints, and N-joint kinematic chain validation (pelvis–spine–extremities) with auxiliary anatomical losses and center-of-mass sanity checks to down-weight physically implausible skeletons.</li>

                <li><strong>Guidance & Training Dynamics:</strong> Uses dual-pass classifier-free guidance (CFG) to trade off diversity vs. adherence to the text prompt, with ablations over guidance strength, noise schedules, and conditioning dropout to study stability, mode-collapse behavior, and semantic fidelity.</li>

                <li><strong>Tooling & Evaluation:</strong> Includes dataset EDA and joint-mapping utilities, pose visualizers for qualitative inspection, and experiment scaffolding to compare baselines (unconditioned / weak conditioning) against the full CLIP-conditioned model under identical sampling budgets.</li>
            </ul>
        </div>

        <div class="external-links">
            <h2>Project Resources</h2>
            <a href="https://github.com/mjsushanth/CLIP-Conditioned-Diffusion-T2Pose-Generation/blob/main/DESIGN_README.md" target="_blank">→ Design Documentation</a>
            <a href="https://github.com/mjsushanth/CLIP-Conditioned-Diffusion-T2Pose-Generation/blob/main/RESEARCH_README.md" target="_blank">→ Research Report</a>
            <a href="https://github.com/mjsushanth/CLIP-Conditioned-Diffusion-T2Pose-Generation" target="_blank">→ GitHub Repository</a>
        </div>
    </div>
</body>
</html>
